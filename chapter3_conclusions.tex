\chapter{Conclusion}

\section{Conclusion}

The study compared the performance of three CNN-based models - baseline CNN, CNN-LSTM, and CNN-Attention - for Fake News classification. Evaluation metrics such as accuracy, precision, recall, and F1 score were used. \\

The CNN-Attention model emerged as the top performer, exhibiting the highest accuracy, precision, and recall. Despite a slightly higher count of false positives compared to baseline \ac{cnn}, it demonstrated superior overall performance in accurately classifying `Fake' and `True' news.\\

In conclusion, the CNN-Attention model stands as the optimal choice for Fake News classification tasks, offering superior accuracy, precision, and recall with 97.14\% accuracy and 97\% precision and recall on the test dataset.

\section{Future Recommendation}

Exploring more recent and advanced \ac{cnn} architectures or hybrid models that combine \ac{cnn} with other neural network structures (e.g., Transformers) could potentially enhance classification accuracy further. Integrating multi-modal data (e.g. text, images, videos) could improve the model's ability to understand and detect fake news. The use of cross-lingual datasets could prevent models from biasing toward specific languages or regions. \\

Furthermore, developing and training a model on Nepali news texts could improve the detection of fake news in the Nepali language, catering to the local population's needs and enhancing the overall reliability of the model in different linguistic scenarios. This would involve creating or curating a large,  labeled dataset of Nepali news articles, both real and fake.